# ğŸ›¡ï¸ SynthShield-AI  
### Build â†’ Break â†’ Improve: Strengthening AI-Generated Image Detection

SynthShield-AI is a deep learning pipeline designed to detect AI-generated (fake) images, evaluate model vulnerabilities through adversarial attacks, and improve robustness using adversarial defense techniques.

This project follows a structured **Build â†’ Break â†’ Improve** methodology to analyze and strengthen AI image detection systems.

---

## ğŸš€ Motivation

With the rapid advancement of generative models, distinguishing between real and AI-generated images has become increasingly difficult. While deep learning detectors achieve high accuracy on clean data, they can still be vulnerable to adversarial manipulation.

SynthShield-AI demonstrates:

- How a high-performing classifier can be fooled
- How adversarial perturbations affect prediction confidence
- How robustness can be improved through adversarial training
- Why explainability matters in AI security systems

---

## ğŸ§© Pipeline Overview

### ğŸ”¹ Phase 1 â€” Build (Baseline Model)

- Dataset: CIFAKE (Real vs AI-generated images)
- Model: Pretrained ResNet18 fine-tuned for binary classification
- Output classes:
  - 0 â†’ FAKE  
  - 1 â†’ REAL  
- Evaluation metrics:
  - Accuracy
  - Confusion matrix
  - Sample predictions

Goal: Train a high-confidence fake image detector capable of distinguishing synthetic content from real images.

---

### ğŸ”¹ Phase 2 â€” Break (Adversarial Attack)

We implemented a **targeted iterative FGSM attack** to flip predictions from FAKE â†’ REAL.

Key steps:
- Select high-confidence FAKE samples
- Apply small, bounded perturbations
- Perform iterative gradient-based updates
- Track fake confidence drop across attack steps
- Visualize adversarial examples

Results:
- Successful prediction flipping
- Significant reduction in fake confidence
- Images remained visually indistinguishable to humans

This phase demonstrates that even accurate models are vulnerable to adversarial manipulation.

---

### ğŸ”¹ Phase 3 â€” Improve (Defense & Robustness)

To strengthen the detector:

- Introduced adversarial training
- Retrained the model using adversarial samples
- Compared baseline vs defended model performance
- Applied Grad-CAM to analyze attention regions

Results:
- Improved resistance to adversarial attacks
- More stable prediction confidence under perturbation
- Better focus on semantically meaningful image regions

This phase shows how robustness techniques can mitigate adversarial vulnerabilities.

---

## ğŸ“Š Key Observations

| Stage     | Outcome |
|-----------|----------|
| Baseline  | High accuracy on clean images |
| Attack    | Successful targeted prediction flipping |
| Defense   | Improved robustness and confidence stability |

The project highlights the importance of adversarial robustness in real-world AI deployment.

---

## ğŸ§  Technical Details

- Backbone: ResNet18 (pretrained on ImageNet)
- Final Layer: Modified for binary classification
- Loss Function: CrossEntropyLoss
- Optimizer: Adam
- Attack Method: Targeted Iterative FGSM
- Defense Strategy: Adversarial training
- Explainability: Grad-CAM visualization

---

## ğŸ“‚ Repository Structure

SynthShield-AI/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ SynthShield.ipynb
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ guardian_stage1.pth
â”‚ â””â”€â”€ guardian_final.pth
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ adversarial_examples.png
â”‚ â”œâ”€â”€ confidence_trajectory.png
â”‚ â”œâ”€â”€ confusion_matrix.png
â”‚ â””â”€â”€ gradcam_example.png
â”‚
â””â”€â”€ report/
â””â”€â”€ SynthShield_Report.pdf


---

## âš™ï¸ Installation & Usage

### 1ï¸âƒ£ Clone the Repository

git clone https://github.com/Vismayavinodk/SynthShield-AI.git
cd SynthShield-AI

### 2ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

### 3ï¸âƒ£ Run the Notebook

jupyter notebook notebooks/SynthShield.ipynb

The notebook contains the complete Build â†’ Break â†’ Improve workflow.

---

## ğŸ” Explainability

Grad-CAM visualizations are used to:

- Interpret model focus regions
- Compare attention before and after adversarial perturbation
- Validate robustness improvements

This adds interpretability to adversarial robustness analysis.

---

## ğŸ¯ Impact

SynthShield-AI demonstrates:

- The fragility of AI detectors under adversarial pressure
- The importance of robustness in synthetic media detection
- A practical evaluation pipeline for secure AI systems

---

## ğŸ‘¥ Team

- Vismaya k
- Vaishnavi V
- Aswathy Menon

---

## ğŸ“œ License

This project is developed for academic and hackathon purposes.

